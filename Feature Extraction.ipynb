{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "SENTENCE_LENGTH_ASSUMPTION=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset count:  2748\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I mean this in a terrible way.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perhaps I caught them on an off night judging ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Later I found myself lost in the power of the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But it is entertaining, nonetheless.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IT'S REALLY EASY.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Speaking of the music, it is unbearably predic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not enough volume.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This place is amazing!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Battery is holding up well.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sorry, I will not be getting food from here an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label\n",
       "0                   I mean this in a terrible way.        0\n",
       "1  Perhaps I caught them on an off night judging ...      0\n",
       "2  Later I found myself lost in the power of the ...      1\n",
       "3             But it is entertaining, nonetheless.        1\n",
       "4                                  IT'S REALLY EASY.      1\n",
       "5  Speaking of the music, it is unbearably predic...      0\n",
       "6                                 Not enough volume.      0\n",
       "7                             This place is amazing!      1\n",
       "8                        Battery is holding up well.      1\n",
       "9  Sorry, I will not be getting food from here an...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon=pandas.read_csv('sentiment labelled sentences\\\\amazon_cells_labelled.txt',sep='\\t',header=None)\n",
    "imdb=pandas.read_csv('sentiment labelled sentences\\\\imdb_labelled.txt',sep='\\t',header=None)\n",
    "yelp=pandas.read_csv('sentiment labelled sentences\\\\yelp_labelled.txt',sep='\\t',header=None)\n",
    "data=pandas.concat([amazon,imdb,yelp])\n",
    "df = shuffle(data,random_state=1) #random_state=seed\n",
    "print(\"Dataset count: \", len(data))\n",
    "data = df.reset_index(drop=True)\n",
    "data.rename(columns={0:'Sentence',1:'Label'}, inplace=True)\n",
    "data.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senticnet Vocab Size:  23682\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Vocab_Scale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa</td>\n",
       "      <td>0.606</td>\n",
       "      <td>-1.731978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aah</td>\n",
       "      <td>-0.510</td>\n",
       "      <td>-1.731831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-0.560</td>\n",
       "      <td>-1.731685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>-1.731539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abase</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-1.731393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abasement</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>-1.731246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abash</td>\n",
       "      <td>-0.540</td>\n",
       "      <td>-1.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abashed</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-1.730954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abashment</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-1.730807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abate</td>\n",
       "      <td>-0.860</td>\n",
       "      <td>-1.730661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Token  Polarity  Vocab_Scale\n",
       "0          aaa     0.606    -1.731978\n",
       "1          aah    -0.510    -1.731831\n",
       "2      abandon    -0.560    -1.731685\n",
       "3  abandonment    -0.650    -1.731539\n",
       "4        abase    -0.580    -1.731393\n",
       "5    abasement    -0.580    -1.731246\n",
       "6        abash    -0.540    -1.731100\n",
       "7      abashed    -0.520    -1.730954\n",
       "8    abashment    -0.600    -1.730807\n",
       "9        abate    -0.860    -1.730661"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentic_net=pandas.read_csv('senticnet\\\\senticnet4.txt',sep='\\\\s+',header=None,usecols=[0,2], names = [\"Token\", \"Polarity\"])\n",
    "sentic_net = sentic_net[~sentic_net['Token'].str.contains('|'.join('_'),na=False)]\n",
    "sentic_net = sentic_net.reset_index(drop=True)\n",
    "print(\"Senticnet Vocab Size: \",len(sentic_net))\n",
    "sentic_net=pandas.concat([sentic_net,pandas.DataFrame(data=scale(list(range(0,23682))),columns=['Vocab_Scale'])],axis=1)\n",
    "\n",
    "sentic_net.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0.15399810070180361,\n",
       " \"''\": -1.4629819566671345,\n",
       " '(': -0.23099715105270544,\n",
       " ')': -0.15399810070180361,\n",
       " ',': 0.0,\n",
       " '--': -1.0009876545617236,\n",
       " '.': -0.076999050350901807,\n",
       " ':': -0.61599240280721446,\n",
       " 'CC': 1.5399810070180362,\n",
       " 'CD': 1.6169800573689381,\n",
       " 'DT': -0.7699905035090181,\n",
       " 'EX': 1.3089838559653308,\n",
       " 'FW': 0.61599240280721446,\n",
       " 'IN': 0.53899335245631264,\n",
       " 'JJ': -1.1549857552635272,\n",
       " 'JJR': 0.7699905035090181,\n",
       " 'JJS': 0.84698955385991992,\n",
       " 'LS': -1.6939791077198398,\n",
       " 'MD': 1.0009876545617236,\n",
       " 'NN': -0.84698955385991992,\n",
       " 'NNP': 1.2319848056144289,\n",
       " 'NNPS': -0.46199430210541087,\n",
       " 'NNS': 1.3859829063162326,\n",
       " 'PDT': 0.92398860421082174,\n",
       " 'POS': 1.6939791077198398,\n",
       " 'PRP': -0.69299145315811628,\n",
       " 'PRP$': -0.38499525175450905,\n",
       " 'RB': 0.23099715105270544,\n",
       " 'RBR': 0.30799620140360723,\n",
       " 'RBS': 0.38499525175450905,\n",
       " 'RP': 0.69299145315811628,\n",
       " 'SYM': 1.4629819566671345,\n",
       " 'TO': -1.6169800573689381,\n",
       " 'UH': -1.3089838559653308,\n",
       " 'VB': 1.0779867049126253,\n",
       " 'VBD': 0.46199430210541087,\n",
       " 'VBG': -1.2319848056144289,\n",
       " 'VBN': -1.5399810070180362,\n",
       " 'VBP': -0.92398860421082174,\n",
       " 'VBZ': -1.0779867049126253,\n",
       " 'WDT': -0.30799620140360723,\n",
       " 'WP': -1.3859829063162326,\n",
       " 'WP$': -0.53899335245631264,\n",
       " 'WRB': 1.1549857552635272,\n",
       " '``': 0.076999050350901807}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagdict = nltk.load('help/tagsets/upenn_tagset.pickle')\n",
    "pos_tags=dict.fromkeys(tagdict.keys())\n",
    "pos_scale=scale(list(range(0,len(pos_tags))))\n",
    "i=0\n",
    "for k,v in pos_tags.items():\n",
    "    pos_tags[k]=pos_scale[i]\n",
    "    i+=1\n",
    "    \n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "#a=[]\n",
    "#S=\"And his subtle connections between the three films are awesome.I have to mention this and it is a huge SPOILER, i loved the ending, how all the characters of the three films were the remaining survivors of the ferry disaster, with Valentine and the young judge together, and the old man watching it on her TV, solidifying his happiness over the suffering which he dealt with for those many years.\"\n",
    "#pos_of_sent=nltk.pos_tag(nltk.word_tokenize(S))\n",
    "#if len(pos_of_sent)>=29:\n",
    "#    print(len(pos_of_sent))\n",
    "#    a=create_feature_vector_of_sentence(pos_of_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('football', 'NN'), ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(\"I love football.\")) ##Example output of pos tag parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector_of_sentence(pos_of_sent):\n",
    "    sentence_feature_vector=np.array([])\n",
    "    word_idx=0\n",
    "    for token, pos_tag in pos_of_sent:\n",
    "        if word_idx>=SENTENCE_LENGTH_ASSUMPTION:\n",
    "            return sentence_feature_vector.reshape(30,3)\n",
    "        word_idx+=1\n",
    "        \n",
    "        if pos_tag not in pos_tags:\n",
    "            print(pos_tag)\n",
    "            word_idx-=1\n",
    "            continue\n",
    "        pos_polarity=pos_tags[pos_tag]\n",
    "        \n",
    "        token_lower=token.lower()\n",
    "        lemma_token=nltk.WordNetLemmatizer().lemmatize(token_lower)        \n",
    "        if token_lower is 'not':\n",
    "            sentence_feature_vector=np.append(sentence_feature_vector,[0,-1,pos_polarity])\n",
    "        elif lemma_token in sentic_net.Token.values:\n",
    "            sentic_net_object=sentic_net.loc[sentic_net.Token == lemma_token]\n",
    "            vocab_score=float(sentic_net_object.Vocab_Scale)\n",
    "            pol_score= float(sentic_net_object.Polarity)\n",
    "            sentence_feature_vector=np.append(sentence_feature_vector,[vocab_score,pol_score,pos_polarity])\n",
    "        else:\n",
    "            sentence_feature_vector=np.append(sentence_feature_vector,[0,0,pos_polarity])\n",
    "        \n",
    "    \n",
    "    for _ in range(0,SENTENCE_LENGTH_ASSUMPTION-word_idx):\n",
    "        sentence_feature_vector=np.append(sentence_feature_vector,[0,0,0])\n",
    "    if len(sentence_feature_vector)==84:\n",
    "        print (str(word_idx)  + \" \" + str(pos_of_sent))\n",
    "    return sentence_feature_vector.reshape(30,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "feature_vectors=np.array([]) #vocab_scale_polarity,sentic_net_polarity,pos_polarity\n",
    "label_vector=np.array([])\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    pos_of_sent=nltk.pos_tag(nltk.word_tokenize(row.Sentence))\n",
    "    feature_vectors=np.append(feature_vectors,create_feature_vector_of_sentence(pos_of_sent))\n",
    "    label_vector=np.append(label_vector,row.Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 30, 3)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=np.array(feature_vectors)\n",
    "f=f.reshape(len(data),30,3)\n",
    "f.dump('features.npx')\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=np.array(label_vector,dtype=int)\n",
    "l.dump('labels.npx')\n",
    "l.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
